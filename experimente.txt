Observatii generale:
-batch size mic la inceput, better
-mai mult de 2048 neuroni=> OOM allocation error
-last dense trebuie sigmoid, altfel cu linear ca in paper ul original obtin loss nan
---- PANA ACUM RESNET DEZGHETAT
----- INGHETAT BASE MODEL!!!
----- pe fine tuning dezghetat blocuri doar
---- adaugat regularizare
---- incercat cu mai putine grid uri -- paper 7x7x30
---- marit input size la 448x448
---- ADAUGAT LAMBDA_CLASS LA FUNCTIA CLASS_LOSS
---- folosit ptr object mask best_iou*true_obj
Recomandari:
-- minim 2000 iteratii * no classes
-- minim 1000-2000 de imagini pe clasa

Experiment 1, 21 feb: 1645470914
-dezghetat tot
-dense head 2048,drop 0.5,elu
-cnt lr 0.00001 
-batch size 8
-100 epoci
-start: loss = 9.63,val_loss = 2.76,mae = 0.17,val_mae=0.155
-end: loss = 0.06, val_loss = 1.48,mae = 0.145,val_mae=0.14
--Observatii: overfit rapid pe la epoca 10

Experiment 2, 22 feb: 1645556152
-dezghetat tot
-schimbat in loss object mask = true_obj cu object mask = best_iou * true_obj
-dense head 2048,drop 0.5,leaky relu 0.1
-cnt lr 0.0001 
-batch size 16
-100 epoci
-start: loss = 4.19,val_loss = 2.11,mae = 0.198,val_mae=0.0811
-end: loss = , val_loss = ,mae = ,val_mae=
overfit rapid, inceput bun

Experiment 3, 22 feb:
-dezghetat tot
-dense head 2048,drop 0.6,leaky relu 0.1
-init lr 0.000001, epoch 60+ lr 0.00001,epoch 100+ lr 0.000001 
-batch size 8
-120 epoci
-start: loss: 17.0641 - mae: 0.4596 - val_loss: 5.4862 - val_mae: 0.3679
-end: loss: 0.0365 - mae: 0.0819 - val_loss: 1.1836 - val_mae: 0.0753
Observatii: 
-pe la epoca 25-30 incepe overfitting ul, la epoca 60 apare un mic salt in antrenare cand cred lr-ul, dar val_loss nu se modifica
-MAE-ul scade constant si se reduce overfitul cand maresc in epoca 60 learning rate-ul 

Experiment 4, 23 feb: 1645614923
-dezghetat tot
-dense head 2048,drop 0.6,leaky relu 0.1
-init lr 0.000001, epoch 30-60 lr 0.0001,epoch 60-100 lr 0.00001,epoch 100+ lr 0.000001 
-batch size 8
-120 epoci
-start:
-end: train_loss:0.03,val_loss=1.49, maetrain=0.083,maeval=0.07
Observatii: La epoca 30 cand cresc lr la 0.0001=> yololoss ul creste f mult si se stabilizeaza pe la epoca 60 cand il cad la 0.00001. Se face overfit ca inainte pe la epoca 30. Varianta asta nu elimina overfitul.

Experiment 5, 8 mar: 
-inghetat baza resnet50
-schimbat no_grids=7 (de la 9 griduri la 7)
-dense head 2048,drop 0.6,leaky relu 0.1
-init lr 0.000001
-batch size 16
-140 epoci
-start: loss: 12.8634 - mae: 0.4491 - val_loss: 4.3763 - val_mae: 0.3497
-end: epoca 40, loss: 2.4692 - mae: 0.1728 - val_loss: 2.2510 - val_mae: 0.1674
Observatii: Parea ca incepe sa faca platou la epoca 40 si lossul era mai mare decat celelalte configuratii la epoca 40

Experiment 6, 8 mar: 
-inghetat baza resnet50
-schimbat no_grids=7 (de la 9 griduri la 7)
-dense head 2048,drop 0.6,leaky relu 0.1
-init lr 0.001
-batch size 32
-160 epoci
-start: 
-end: 
Observatii:  nan


Experiment 7, 8 mar: 
-inghetat baza resnet50
-schimbat no_grids=7 (de la 9 griduri la 7)
-dense head 2048,drop 0.6,leaky relu 0.1
-init lr 0.0001
-batch size 32
-160 epoci
-start: 
-end: 
Observatii:  platou la primele epoci


Experiment 8, 8 mar: 
-inghetat baza resnet50
-schimbat no_grids=7 (de la 9 griduri la 7)
-dense head 4096,drop 0.6,leaky relu 0.1
-init lr 0.0001
-batch size 32
-160 epoci
-start: 
-end: loss: 1.9843 - mae: 0.1206 - val_loss: 2.5690 - val_mae: 0.1125
Observatii:  nu invata

Experiment 9, 9 mar: 
-inghetat baza resnet50
-schimbat no_grids=7 (de la 9 griduri la 7)
-dense head 4096,drop 0.6,leaky relu 0.1
-init lr 0.00001
-batch size 32
-160 epoci
-start: 
-end: 
Observatii:  nu pare bine

Experiment 9, 9 mar: 
-inghetat baza resnet50
-schimbat no_grids=7 (de la 9 griduri la 7)
-dense head 4096,drop 0.6,leaky relu 0.1
-init lr 0.00001
-batch size 32
-160 epoci
-start: 
-end: 
Observatii:  nu pare bine


Experiment 10, 9 mar: 
-inghetat baza resnet50
-schimbat no_grids=7/9/11 (de la 9 griduri la 7)
-dense head 2048,drop 0.5,leaky relu 0.1,linear output
-init lr 0.001/0.0001/0.00001/0.000001
-batch size 64
-15 epoci
-start: 
-end: nan

Experiment 11 , 8 mar: 
-inghetat baza resnet50
-lambda noobj si coord schimbat la 10
-schimbat no_grids=9-dense head 2048,drop 0.5,leaky relu 0.1
-init lr 0.0003 
-batch size 16
-160 epoci
-start:
-end: nu invata 

Experiment 12 , 10 mar: 
-inghetat baza resnet50
-lambda noobj si coord schimbat la 10
-schimbat no_grids=9-dense head 4096,drop 0.5,leaky relu 0.1
-init lr 0.0003 pana la 70,70-200 0.00001, 200+ 0.000001
-batch size 16
-300 epoci
-start:
-end: ramane blocat la 2.3,


Experiment 11 ,10 mar: 
-inghetat baza resnet50
-lambda noobj si coord schimbat la 10
-schimbat no_grids=9-dense head 4096,drop 0.5,leaky relu 0.1
-init lr 0.001
-batch size 32
-15 epoci
-start:
-end: ramane blocat la 2.7 f repede

Experiment 12 ,10 mar: 
-inghetat baza resnet50
-lambda noobj si coord schimbat la 0.5 si 5
-schimbat no_grids=9-dense head 4096,leaky relu 0.1
-init lr 0.001/0.0001
-batch size 32
-15 epoci
-start:
-end: 0.001,0.0001-- nu invata blocat la loss=5 ambele

Recomandari: incercat next cu batch size de 32, apoi revenit la batch size mic si lr de 0.0001/0.001

Links:

https://medium.com/@m.khan/implementing-yolo-using-resnet-as-feature-extractor-5857f9da5014

https://github.com/makatx/YOLO_ResNet/blob/8f7b593e5232f8038a72eb52e920a97c207dd57c/model_continue_train.py#L294

https://github.com/thtrieu/darkflow/issues/9
