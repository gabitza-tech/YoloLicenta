Observatii generale:
-batch size mic la inceput, better
-mai mult de 2048 neuroni=> OOM allocation error
-last dense trebuie sigmoid, altfel cu linear ca in paper ul original obtin loss nan
---- PANA ACUM RESNET DEZGHETAT
----- INGHETAT BASE MODEL!!!
----- pe fine tuning dezghetat blocuri doar
---- adaugat regularizare
---- incercat cu mai putine grid uri -- paper 7x7x30
---- marit input size la 448x448
---- ADAUGAT LAMBDA_CLASS LA FUNCTIA CLASS_LOSS
---- folosit ptr object mask best_iou*true_obj
Recomandari:
-- minim 2000 iteratii * no classes
-- minim 1000-2000 de imagini pe clasa

Experiment 1, 21 feb: 1645470914
-dezghetat tot
-dense head 2048,drop 0.5,elu
-cnt lr 0.00001 
-batch size 8
-100 epoci
-start: loss = 9.63,val_loss = 2.76,mae = 0.17,val_mae=0.155
-end: loss = 0.06, val_loss = 1.48,mae = 0.145,val_mae=0.14
--Observatii: overfit rapid pe la epoca 10

Experiment 2, 22 feb: 1645556152
-dezghetat tot
-schimbat in loss object mask = true_obj cu object mask = best_iou * true_obj
-dense head 2048,drop 0.5,leaky relu 0.1
-cnt lr 0.0001 
-batch size 16
-100 epoci
-start: loss = 4.19,val_loss = 2.11,mae = 0.198,val_mae=0.0811
-end: loss = , val_loss = ,mae = ,val_mae=
overfit rapid, inceput bun

Experiment 3, 22 feb:
-dezghetat tot
-dense head 2048,drop 0.6,leaky relu 0.1
-init lr 0.000001, epoch 60+ lr 0.00001,epoch 100+ lr 0.000001 
-batch size 8
-120 epoci
-start: loss: 17.0641 - mae: 0.4596 - val_loss: 5.4862 - val_mae: 0.3679
-end: loss: 0.0365 - mae: 0.0819 - val_loss: 1.1836 - val_mae: 0.0753
Observatii: 
-pe la epoca 25-30 incepe overfitting ul, la epoca 60 apare un mic salt in antrenare cand cred lr-ul, dar val_loss nu se modifica
-MAE-ul scade constant si se reduce overfitul cand maresc in epoca 60 learning rate-ul 

Experiment 4, 23 feb: 1645614923
-dezghetat tot
-dense head 2048,drop 0.6,leaky relu 0.1
-init lr 0.000001, epoch 30-60 lr 0.0001,epoch 60-100 lr 0.00001,epoch 100+ lr 0.000001 
-batch size 8
-120 epoci
-start:
-end: train_loss:0.03,val_loss=1.49, maetrain=0.083,maeval=0.07
Observatii: La epoca 30 cand cresc lr la 0.0001=> yololoss ul creste f mult si se stabilizeaza pe la epoca 60 cand il cad la 0.00001. Se face overfit ca inainte pe la epoca 30. Varianta asta nu elimina overfitul.

Experiment 5, 8 mar: 
-inghetat baza resnet50
-schimbat no_grids=7 (de la 9 griduri la 7)
-dense head 2048,drop 0.6,leaky relu 0.1
-init lr 0.000001
-batch size 16
-140 epoci
-start: loss: 12.8634 - mae: 0.4491 - val_loss: 4.3763 - val_mae: 0.3497
-end: epoca 40, loss: 2.4692 - mae: 0.1728 - val_loss: 2.2510 - val_mae: 0.1674
Observatii: Parea ca incepe sa faca platou la epoca 40 si lossul era mai mare decat celelalte configuratii la epoca 40

Experiment 6, 8 mar: 
-inghetat baza resnet50
-schimbat no_grids=7 (de la 9 griduri la 7)
-dense head 2048,drop 0.6,leaky relu 0.1
-init lr 0.001
-batch size 32
-160 epoci
-start: 
-end: 
Observatii:  nan


Experiment 7, 8 mar: 
-inghetat baza resnet50
-schimbat no_grids=7 (de la 9 griduri la 7)
-dense head 2048,drop 0.6,leaky relu 0.1
-init lr 0.0001
-batch size 32
-160 epoci
-start: 
-end: 
Observatii:  platou la primele epoci


Experiment 8, 8 mar: 
-inghetat baza resnet50
-schimbat no_grids=7 (de la 9 griduri la 7)
-dense head 4096,drop 0.6,leaky relu 0.1
-init lr 0.0001
-batch size 32
-160 epoci
-start: 
-end: loss: 1.9843 - mae: 0.1206 - val_loss: 2.5690 - val_mae: 0.1125
Observatii:  nu invata

Experiment 9, 9 mar: 
-inghetat baza resnet50
-schimbat no_grids=7 (de la 9 griduri la 7)
-dense head 4096,drop 0.6,leaky relu 0.1
-init lr 0.00001
-batch size 32
-160 epoci
-start: 
-end: 
Observatii:  nu pare bine

Experiment 9, 9 mar: 
-inghetat baza resnet50
-schimbat no_grids=7 (de la 9 griduri la 7)
-dense head 4096,drop 0.6,leaky relu 0.1
-init lr 0.00001
-batch size 32
-160 epoci
-start: 
-end: 
Observatii:  nu pare bine


Experiment 10, 9 mar: 
-inghetat baza resnet50
-schimbat no_grids=7/9/11 (de la 9 griduri la 7)
-dense head 2048,drop 0.5,leaky relu 0.1,linear output
-init lr 0.001/0.0001/0.00001/0.000001
-batch size 64
-15 epoci
-start: 
-end: nan

Experiment 11 , 8 mar: 
-inghetat baza resnet50
-lambda noobj si coord schimbat la 10
-schimbat no_grids=9-dense head 2048,drop 0.5,leaky relu 0.1
-init lr 0.0003 
-batch size 16
-160 epoci
-start:
-end: nu invata 

Experiment 12 , 10 mar: 
-inghetat baza resnet50
-lambda noobj si coord schimbat la 10
-schimbat no_grids=9-dense head 4096,drop 0.5,leaky relu 0.1
-init lr 0.0003 pana la 70,70-200 0.00001, 200+ 0.000001
-batch size 16
-300 epoci
-start:
-end: ramane blocat la 2.3,


Experiment 11 ,10 mar: 
-inghetat baza resnet50
-lambda noobj si coord schimbat la 10
-schimbat no_grids=9-dense head 4096,drop 0.5,leaky relu 0.1
-init lr 0.001
-batch size 32
-15 epoci
-start:
-end: ramane blocat la 2.7 f repede

Experiment 12 ,10 mar: 
-inghetat baza resnet50
-lambda noobj si coord schimbat la 0.5 si 5
-schimbat no_grids=9-dense head 4096,leaky relu 0.1
-init lr 0.001/0.0001
-batch size 32
-15 epoci
-start:
-end: 0.001,0.0001-- nu invata blocat la loss=5 ambele


Experiment 13 ,10 mar: 
-inghetat baza resnet50
-lambda noobj si coord schimbat la 0.5 si 5
-schimbat no_grids=9-dense head 4096,leaky relu 0.1
-init lr 0.00001
-batch size 32
-350-400 epoci + 100
-start:
-end: loss: 0.0937 - mae: 0.1189 - val_loss: 1.8871 - val_mae: 0.1196

Recomandari: incercat next cu batch size de 32, apoi revenit la batch size mic si lr de 0.0001/0.001

Experiment 14 ,10-16 mar: 
-inghetat baza resnet50
-lambda noobj si coord schimbat la 0.5 si 5, obj la 5
-schimbat no_grids=7-dense head 2048,leaky relu 0.1
-learning_rate=0.5e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0
-no drop
-batch size 16
-400 epoci 
-start:
-end: loss: loss: 0.3184 - mae: 0.1230 - val_loss: 3.3264 - val_mae: 0.1260

Experiment 15 ,22 mar: 
-inghetat baza resnet50
-lambda noobj si coord schimbat la 1 si 5, obj la 1,cls 2
-schimbat no_grids=7-dense head 2048,leaky relu 0.1, drop 0.5
-learning_rate=1e-6 constant
-batch size 8
-64 epoci 
-start:
-end: loss:  6.5772 - mae: 0.1643 - val_loss: 6.3547 - val_mae: 0.1623
/// continuare fine tune
- dezghetat tot
- totu la fel 
- 192 epoci
- end:  loss: 2.3658 - mae: 0.1480 - val_loss: 5.5707 - val_mae: 0.1375

Experiment 16, 24.03:
- dezghetat tot, 2048 dense, leaky 0.1, drop 0.6, output linear
-no_grids 7
- schimbat functia loss: ptr bbox si confidence am pus tf.nn.sigmoid ca transformare si pentru clase am pus tf.nn.softmax, in rest am pastrat calculul loss-ului la fel
- batch 8 
- epoci 100
-lr 0.00001
- end: Epoch 00099: loss did not improve from 8.87421
Epoch 100/250
1542/1542 [==============================] - 459s 298ms/step - loss: 8.6825 - mae: 1.4237 - val_loss: 47.8476 - val_mae: 1.3022
!!!!BEST!!!!
Experiment 17, 24.03:
- dezghetat tot, 2048 dense, leaky 0.1, drop 0.6, output sigmoid
-no_grids 7
-train/test split 0.8/0.2
- functie loss normala cu sum peste toate imaginile din batch
- batch 16
- epoci 250
- lr 0.00001 ( cu 0.001,0.0001 nu a mers prea bine, scadea incet )
- end: 293s 428ms/step - loss: 3.3784 - mae: 0.1380 - val_loss: 68.3267 - val_mae: 0.1349
Epoch 00250: loss did not improve from 3.35610
OBS:- Desi val_loss-ul e mult mai mare decat loss-ul pe train, predictiile sunt foarte bune si precise in general. Class score-ul este destul de mic totusi si ar trebui sa ma concentrez sa obtin o eroare de clasificare mai mica. 
- inferenta se face cu 13-14FPS in medie
- am adaugt Non Maximum Suppression
Experiment 18,29.03:
- am schimbat lr= 0.000005
- am mai lasat 250 de epoci
- a scazut la 2.0 training loss ul si a crescut la 69 val loss ul
- nicio imbunatatire in predictii, actually sunt mai rele

Experiment 19,30.03:
- dezghetat doar head for warm up,2048 dense, leaky 0.1, drop 0.6, output sigmoid
- facut mean pe loss final la dimensiune batch
- init lr = 0.0001
-batch 32
-100 epoci
-end: slab: overfit imediat, nu trece de 5.7 val loss-ul 

Experiment 20,31.03:
- dezghetat doar head for warm up,4096 dense, leaky 0.1, drop 0.6, output sigmoid
- facut mean pe loss final la dimensiune batch
- init lr = 0.001(nan loss),0.0001(nu invata),reduce lr on plateau, patience= 6, factor = 0.1,metrics = val_loss
-batch 32
-100 epoci
-niciun rezultat ok in primele epoci

Experiment 21,31.03: (reincercat antrenat apropiat de cel mai bun model incercat)
- warm cu head doar, 2048 dense, !mish activation, drop 0.6, output sigmoid
-no_grids 7
-train/test split 0.72train/0.18val/0.1 test
- mean pe batch
- batch 16
- lr = 0.0001, reduce lr on plateau, patience= 6, factor = 0.1,metrics = val_loss,early stop patience 10
-early stop epoch 17 5.52 val_loss

Experiment 22,31.03: (reincercat antrenat apropiat de cel mai bun model incercat)
- warm cu head doar, 2048 dense, !mish activation, drop 0.6, output sigmoid
-no_grids 7
-train/test split 0.72train/0.18val/0.1 test
- mean pe batch
- batch 16
- lr = 0.0005(nan), reduce lr on plateau, patience= 6, factor = 0.2,metrics = val_loss,early stop patience 10
- batch 64+lr 0.00005 --end 5.15 val_loss, epoch ~65-70

Experiment 23,31.03: (reincercat antrenat apropiat de cel mai bun model incercat)
- warm cu head doar, 4096 dense, !mish activation, drop 0.5, output sigmoid
-no_grids 7
-train/test split 0.72train/0.18val/0.1 test
- mean pe batch
- batch 64
- lr = 0.00005, reduce lr on plateau, patience= 6, factor = 0.2,metrics = val_loss,early stop patience 10
- end: val_loss 5.05 epoch ~89, last learning rate 3*10^7
Experiment 24:
-dezghetat cap+un block( layers[-36:]=True)
-folosit warm 5.05 si ultima configuratie din exp 23
-lr = 0.0001 si tot restu la fel
-150 de epoci
-epoch 19 end 5.15 
Exp 25:
-batch 8
-lr = 0.00001
-end epoch 19:loss: 2.5616 - mae: 0.1365 - val_loss: 5.1225 - val_mae: 0.1342
Exp 26:
-batch 8 and lr = 0.000001
-epoch 25, loss: 3.9141 - mae: 0.1336 - val_loss: 5.2456 - val_mae: 0.1303
Exp 27:
-dezghetat doua blocuri [-98:],fol warm 5.05
-lr 0.0001(nan),batch 16(oom allocation error), batch 8
- lr 0.00001 and batch 8
-epoch 65 end:- 935s 607ms/step - loss: 0.4730 - mae: 0.1353 - val_loss: 2.9032 - val_mae: 0.1203
Exp 28:
-inghetat intreg resnet, 224 input
-leaky 0.2, batch 16,drop0.7, loss class_lambda=1,9 grids
-epoc <60 lr = 10^-5, epoch > 60 lr = 10^-6
-epoch 113:  loss: 3.2760 - mae: 0.1532 - val_loss: 3.9125 - val_mae: 0.1508
Continuare:
-micsorat batch la  8
-marit lr la 10^-5
-antrenat inca 100 epoci tot in warm up
NEXT:
- reincercat cu lr de 0.001 si 0.0001 sa vad daca merge in nan si sa scad treptat lr-ul cu Reduce on Plateau
-marit training/test ratio 0.9/0.1 (DONE)
-regularizari
-sa antrenez ptr 250 de epoci si cu 9/11 griduri
-sa antrenez mai mult de 250 de epoci cu aceiasi configuratie
-schimb din sum ptr toate imaginile in MEAN !!
-sa incerc sa reduc overfit-ul
-sa adaug mAP, sa imbunatatesc algoritmul de NMS
-sa adaug ancore
-
-incercat cu 4096 neuroni
- facut warm si obtinut un model cu loss cat mai mic si dupa fine tune si dezghetat pe rand cate un bloc/doua/trei/toate reteaua

OBS: AM SCHIMBAT IN TRANSFORMS DE LA 25 la 30 forma sa fie la fel ca de la 21 la 25
Links:

https://medium.com/@m.khan/implementing-yolo-using-resnet-as-feature-extractor-5857f9da5014

https://github.com/makatx/YOLO_ResNet/blob/8f7b593e5232f8038a72eb52e920a97c207dd57c/model_continue_train.py#L294

https://github.com/thtrieu/darkflow/issues/9
