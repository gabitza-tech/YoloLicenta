Observatii generale:
-batch size mic la inceput, better
-mai mult de 2048 neuroni=> OOM allocation error
-last dense trebuie sigmoid, altfel cu linear ca in paper ul original obtin loss nan
---- PANA ACUM RESNET DEZGHETAT
----- INGHETAT BASE MODEL!!!
----- pe fine tuning dezghetat blocuri doar
---- adaugat regularizare
---- incercat cu mai putine grid uri -- paper 7x7x30
---- marit input size la 448x448
---- ADAUGAT LAMBDA_CLASS LA FUNCTIA CLASS_LOSS
---- folosit ptr object mask best_iou*true_obj
Recomandari:
-- minim 2000 iteratii * no classes
-- minim 1000-2000 de imagini pe clasa

Experiment 1, 21 feb: 1645470914
-dezghetat tot
-dense head 2048,drop 0.5,elu
-cnt lr 0.00001 
-batch size 8
-100 epoci
-start: loss = 9.63,val_loss = 2.76,mae = 0.17,val_mae=0.155
-end: loss = 0.06, val_loss = 1.48,mae = 0.145,val_mae=0.14
--Observatii: overfit rapid pe la epoca 10

Experiment 2, 22 feb: 1645556152
-dezghetat tot
-schimbat in loss object mask = true_obj cu object mask = best_iou * true_obj
-dense head 2048,drop 0.5,leaky relu 0.1
-cnt lr 0.0001 
-batch size 16
-100 epoci
-start: loss = 4.19,val_loss = 2.11,mae = 0.198,val_mae=0.0811
-end: loss = , val_loss = ,mae = ,val_mae=
overfit rapid, inceput bun

Experiment 3, 22 feb:
-dezghetat tot
-dense head 2048,drop 0.6,leaky relu 0.1
-init lr 0.000001, epoch 60+ lr 0.00001,epoch 100+ lr 0.000001 
-batch size 8
-120 epoci
-start: loss: 17.0641 - mae: 0.4596 - val_loss: 5.4862 - val_mae: 0.3679
-end: loss: 0.0365 - mae: 0.0819 - val_loss: 1.1836 - val_mae: 0.0753
Observatii: 
-pe la epoca 25-30 incepe overfitting ul, la epoca 60 apare un mic salt in antrenare cand cred lr-ul, dar val_loss nu se modifica
-MAE-ul scade constant si se reduce overfitul cand maresc in epoca 60 learning rate-ul 

Experiment 4, 23 feb: 1645614923
-dezghetat tot
-dense head 2048,drop 0.6,leaky relu 0.1
-init lr 0.000001, epoch 30-60 lr 0.0001,epoch 60-100 lr 0.00001,epoch 100+ lr 0.000001 
-batch size 8
-120 epoci
-start:
-end: train_loss:0.03,val_loss=1.49, maetrain=0.083,maeval=0.07
Observatii: La epoca 30 cand cresc lr la 0.0001=> yololoss ul creste f mult si se stabilizeaza pe la epoca 60 cand il cad la 0.00001. Se face overfit ca inainte pe la epoca 30. Varianta asta nu elimina overfitul.

Experiment 5, 8 mar: 
-inghetat baza resnet50
-schimbat no_grids=7 (de la 9 griduri la 7)
-dense head 2048,drop 0.6,leaky relu 0.1
-init lr 0.000001
-batch size 16
-140 epoci
-start: loss: 12.8634 - mae: 0.4491 - val_loss: 4.3763 - val_mae: 0.3497
-end: epoca 40, loss: 2.4692 - mae: 0.1728 - val_loss: 2.2510 - val_mae: 0.1674
Observatii: Parea ca incepe sa faca platou la epoca 40 si lossul era mai mare decat celelalte configuratii la epoca 40

Experiment 6, 8 mar: 
-inghetat baza resnet50
-schimbat no_grids=7 (de la 9 griduri la 7)
-dense head 2048,drop 0.6,leaky relu 0.1
-init lr 0.001
-batch size 32
-160 epoci
-start: 
-end: 
Observatii:  nan


Experiment 7, 8 mar: 
-inghetat baza resnet50
-schimbat no_grids=7 (de la 9 griduri la 7)
-dense head 2048,drop 0.6,leaky relu 0.1
-init lr 0.0001
-batch size 32
-160 epoci
-start: 
-end: 
Observatii:  platou la primele epoci


Experiment 8, 8 mar: 
-inghetat baza resnet50
-schimbat no_grids=7 (de la 9 griduri la 7)
-dense head 4096,drop 0.6,leaky relu 0.1
-init lr 0.0001
-batch size 32
-160 epoci
-start: 
-end: loss: 1.9843 - mae: 0.1206 - val_loss: 2.5690 - val_mae: 0.1125
Observatii:  nu invata

Experiment 9, 9 mar: 
-inghetat baza resnet50
-schimbat no_grids=7 (de la 9 griduri la 7)
-dense head 4096,drop 0.6,leaky relu 0.1
-init lr 0.00001
-batch size 32
-160 epoci
-start: 
-end: 
Observatii:  nu pare bine

Experiment 9, 9 mar: 
-inghetat baza resnet50
-schimbat no_grids=7 (de la 9 griduri la 7)
-dense head 4096,drop 0.6,leaky relu 0.1
-init lr 0.00001
-batch size 32
-160 epoci
-start: 
-end: 
Observatii:  nu pare bine


Experiment 10, 9 mar: 
-inghetat baza resnet50
-schimbat no_grids=7/9/11 (de la 9 griduri la 7)
-dense head 2048,drop 0.5,leaky relu 0.1,linear output
-init lr 0.001/0.0001/0.00001/0.000001
-batch size 64
-15 epoci
-start: 
-end: nan

Experiment 11 , 8 mar: 
-inghetat baza resnet50
-lambda noobj si coord schimbat la 10
-schimbat no_grids=9-dense head 2048,drop 0.5,leaky relu 0.1
-init lr 0.0003 
-batch size 16
-160 epoci
-start:
-end: nu invata 

Experiment 12 , 10 mar: 
-inghetat baza resnet50
-lambda noobj si coord schimbat la 10
-schimbat no_grids=9-dense head 4096,drop 0.5,leaky relu 0.1
-init lr 0.0003 pana la 70,70-200 0.00001, 200+ 0.000001
-batch size 16
-300 epoci
-start:
-end: ramane blocat la 2.3,


Experiment 11 ,10 mar: 
-inghetat baza resnet50
-lambda noobj si coord schimbat la 10
-schimbat no_grids=9-dense head 4096,drop 0.5,leaky relu 0.1
-init lr 0.001
-batch size 32
-15 epoci
-start:
-end: ramane blocat la 2.7 f repede

Experiment 12 ,10 mar: 
-inghetat baza resnet50
-lambda noobj si coord schimbat la 0.5 si 5
-schimbat no_grids=9-dense head 4096,leaky relu 0.1
-init lr 0.001/0.0001
-batch size 32
-15 epoci
-start:
-end: 0.001,0.0001-- nu invata blocat la loss=5 ambele


Experiment 13 ,10 mar: 
-inghetat baza resnet50
-lambda noobj si coord schimbat la 0.5 si 5
-schimbat no_grids=9-dense head 4096,leaky relu 0.1
-init lr 0.00001
-batch size 32
-350-400 epoci + 100
-start:
-end: loss: 0.0937 - mae: 0.1189 - val_loss: 1.8871 - val_mae: 0.1196

Recomandari: incercat next cu batch size de 32, apoi revenit la batch size mic si lr de 0.0001/0.001

Experiment 14 ,10-16 mar: 
-inghetat baza resnet50
-lambda noobj si coord schimbat la 0.5 si 5, obj la 5
-schimbat no_grids=7-dense head 2048,leaky relu 0.1
-learning_rate=0.5e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0
-no drop
-batch size 16
-400 epoci 
-start:
-end: loss: loss: 0.3184 - mae: 0.1230 - val_loss: 3.3264 - val_mae: 0.1260

Experiment 15 ,22 mar: 
-inghetat baza resnet50
-lambda noobj si coord schimbat la 1 si 5, obj la 1,cls 2
-schimbat no_grids=7-dense head 2048,leaky relu 0.1, drop 0.5
-learning_rate=1e-6 constant
-batch size 8
-64 epoci 
-start:
-end: loss:  6.5772 - mae: 0.1643 - val_loss: 6.3547 - val_mae: 0.1623
/// continuare fine tune
- dezghetat tot
- totu la fel 
- 192 epoci
- end:  loss: 2.3658 - mae: 0.1480 - val_loss: 5.5707 - val_mae: 0.1375

Experiment 16, 24.03:
- dezghetat tot, 2048 dense, leaky 0.1, drop 0.6, output linear
-no_grids 7
- schimbat functia loss: ptr bbox si confidence am pus tf.nn.sigmoid ca transformare si pentru clase am pus tf.nn.softmax, in rest am pastrat calculul loss-ului la fel
- batch 8 
- epoci 100
-lr 0.00001
- end: Epoch 00099: loss did not improve from 8.87421
Epoch 100/250
1542/1542 [==============================] - 459s 298ms/step - loss: 8.6825 - mae: 1.4237 - val_loss: 47.8476 - val_mae: 1.3022

Experiment 17, 24.03:
- dezghetat tot, 2048 dense, leaky 0.1, drop 0.6, output sigmoid
-no_grids 7
- functie loss normala cu sum peste toate imaginile
- batch 16
- epoci 
- lr 0.00001 ( cu 0.001,0.0001 nu a mers prea bine, scadea incet )
- end: 
NEXT:
-incercat sa schimb in loss object_delta si sa schimb din sum ptr toate imaginile in MEAN !!
-incercat cu 4096 neuroni
- facut warm si obtinut un model cu loss cat mai mic si dupa fine tune si dezghetat pe rand cate un bloc/doua/trei/toate reteaua

OBS: AM SCHIMBAT IN TRANSFORMS DE LA 25 la 30 forma sa fie la fel ca de la 21 la 25
Links:

https://medium.com/@m.khan/implementing-yolo-using-resnet-as-feature-extractor-5857f9da5014

https://github.com/makatx/YOLO_ResNet/blob/8f7b593e5232f8038a72eb52e920a97c207dd57c/model_continue_train.py#L294

https://github.com/thtrieu/darkflow/issues/9
