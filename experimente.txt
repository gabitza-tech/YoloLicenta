Observatii generale:
-batch size mic la inceput, better
-mai mult de 2048 neuroni=> OOM allocation error
-last dense trebuie sigmoid, altfel cu linear ca in paper ul original obtin loss nan
---- PANA ACUM RESNET DEZGHETAT
----- INGHETAT BASE MODEL!!!
----- pe fine tuning dezghetat blocuri doar
---- adaugat regularizare
---- incercat cu mai putine grid uri -- paper 7x7x30
---- marit input size la 448x448
---- ADAUGAT LAMBDA_CLASS LA FUNCTIA CLASS_LOSS
---- folosit ptr object mask best_iou*true_obj
Recomandari:
-- minim 2000 iteratii * no classes
-- minim 1000-2000 de imagini pe clasa

Experiment 1, 21 feb: 1645470914
-dezghetat tot
-dense head 2048,drop 0.5,elu
-cnt lr 0.00001 
-batch size 8
-100 epoci
-start: loss = 9.63,val_loss = 2.76,mae = 0.17,val_mae=0.155
-end: loss = 0.06, val_loss = 1.48,mae = 0.145,val_mae=0.14
--Observatii: overfit rapid pe la epoca 10

Experiment 2, 22 feb: 1645556152
-dezghetat tot
-schimbat in loss object mask = true_obj cu object mask = best_iou * true_obj
-dense head 2048,drop 0.5,leaky relu 0.1
-cnt lr 0.0001 
-batch size 16
-100 epoci
-start: loss = 4.19,val_loss = 2.11,mae = 0.198,val_mae=0.0811
-end: loss = , val_loss = ,mae = ,val_mae=
overfit rapid, inceput bun

Experiment 3, 22 feb:
-dezghetat tot
-dense head 2048,drop 0.6,leaky relu 0.1
-init lr 0.000001, epoch 60+ lr 0.00001,epoch 100+ lr 0.000001 
-batch size 8
-120 epoci
-start: loss: 17.0641 - mae: 0.4596 - val_loss: 5.4862 - val_mae: 0.3679
-end: loss: 0.0365 - mae: 0.0819 - val_loss: 1.1836 - val_mae: 0.0753
Observatii: 
-pe la epoca 25-30 incepe overfitting ul, la epoca 60 apare un mic salt in antrenare cand cred lr-ul, dar val_loss nu se modifica
-MAE-ul scade constant si se reduce overfitul cand maresc in epoca 60 learning rate-ul 

Experiment 4, 23 feb: 1645614923
-dezghetat tot
-dense head 2048,drop 0.6,leaky relu 0.1
-init lr 0.000001, epoch 30-60 lr 0.0001,epoch 60-100 lr 0.00001,epoch 100+ lr 0.000001 
-batch size 8
-120 epoci
-start:
-end: train_loss:0.03,val_loss=1.49, maetrain=0.083,maeval=0.07
Observatii: La epoca 30 cand cresc lr la 0.0001=> yololoss ul creste f mult si se stabilizeaza pe la epoca 60 cand il cad la 0.00001. Se face overfit ca inainte pe la epoca 30. Varianta asta nu elimina overfitul.

Experiment 5, 8 mar: 
-inghetat baza resnet50
-schimbat no_grids=7 (de la 9 griduri la 7)
-dense head 2048,drop 0.6,leaky relu 0.1
-init lr 0.000001
-batch size 16
-140 epoci
-start: loss: 12.8634 - mae: 0.4491 - val_loss: 4.3763 - val_mae: 0.3497
-end: epoca 40, loss: 2.4692 - mae: 0.1728 - val_loss: 2.2510 - val_mae: 0.1674
Observatii: Parea ca incepe sa faca platou la epoca 40 si lossul era mai mare decat celelalte configuratii la epoca 40

Experiment 6, 8 mar: 
-inghetat baza resnet50
-schimbat no_grids=7 (de la 9 griduri la 7)
-dense head 2048,drop 0.6,leaky relu 0.1
-init lr 0.001
-batch size 32
-160 epoci
-start: 
-end: 
Observatii:  nan


Experiment 7, 8 mar: 
-inghetat baza resnet50
-schimbat no_grids=7 (de la 9 griduri la 7)
-dense head 2048,drop 0.6,leaky relu 0.1
-init lr 0.0001
-batch size 32
-160 epoci
-start: 
-end: 
Observatii:  platou la primele epoci


Experiment 8, 8 mar: 
-inghetat baza resnet50
-schimbat no_grids=7 (de la 9 griduri la 7)
-dense head 4096,drop 0.6,leaky relu 0.1
-init lr 0.0001
-batch size 32
-160 epoci
-start: 
-end: loss: 1.9843 - mae: 0.1206 - val_loss: 2.5690 - val_mae: 0.1125
Observatii:  nu invata

Experiment 9, 9 mar: 
-inghetat baza resnet50
-schimbat no_grids=7 (de la 9 griduri la 7)
-dense head 4096,drop 0.6,leaky relu 0.1
-init lr 0.00001
-batch size 32
-160 epoci
-start: 
-end: 
Observatii:  nu pare bine

Experiment 9, 9 mar: 
-inghetat baza resnet50
-schimbat no_grids=7 (de la 9 griduri la 7)
-dense head 4096,drop 0.6,leaky relu 0.1
-init lr 0.00001
-batch size 32
-160 epoci
-start: 
-end: 
Observatii:  nu pare bine


Experiment 10, 9 mar: 
-inghetat baza resnet50
-schimbat no_grids=7/9/11 (de la 9 griduri la 7)
-dense head 2048,drop 0.5,leaky relu 0.1,linear output
-init lr 0.001/0.0001/0.00001/0.000001
-batch size 64
-15 epoci
-start: 
-end: nan

Experiment 11 , 8 mar: 
-inghetat baza resnet50
-lambda noobj si coord schimbat la 10
-schimbat no_grids=9-dense head 2048,drop 0.5,leaky relu 0.1
-init lr 0.0003 
-batch size 16
-160 epoci
-start:
-end: nu invata 

Experiment 12 , 10 mar: 
-inghetat baza resnet50
-lambda noobj si coord schimbat la 10
-schimbat no_grids=9-dense head 4096,drop 0.5,leaky relu 0.1
-init lr 0.0003 pana la 70,70-200 0.00001, 200+ 0.000001
-batch size 16
-300 epoci
-start:
-end: ramane blocat la 2.3,


Experiment 11 ,10 mar: 
-inghetat baza resnet50
-lambda noobj si coord schimbat la 10
-schimbat no_grids=9-dense head 4096,drop 0.5,leaky relu 0.1
-init lr 0.001
-batch size 32
-15 epoci
-start:
-end: ramane blocat la 2.7 f repede

Experiment 12 ,10 mar: 
-inghetat baza resnet50
-lambda noobj si coord schimbat la 0.5 si 5
-schimbat no_grids=9-dense head 4096,leaky relu 0.1
-init lr 0.001/0.0001
-batch size 32
-15 epoci
-start:
-end: 0.001,0.0001-- nu invata blocat la loss=5 ambele


Experiment 13 ,10 mar: 
-inghetat baza resnet50
-lambda noobj si coord schimbat la 0.5 si 5
-schimbat no_grids=9-dense head 4096,leaky relu 0.1
-init lr 0.00001
-batch size 32
-350-400 epoci + 100
-start:
-end: loss: 0.0937 - mae: 0.1189 - val_loss: 1.8871 - val_mae: 0.1196

Recomandari: incercat next cu batch size de 32, apoi revenit la batch size mic si lr de 0.0001/0.001

Experiment 14 ,10-16 mar: 
-inghetat baza resnet50
-lambda noobj si coord schimbat la 0.5 si 5, obj la 5
-schimbat no_grids=7-dense head 2048,leaky relu 0.1
-learning_rate=0.5e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0
-no drop
-batch size 16
-400 epoci 
-start:
-end: loss: loss: 0.3184 - mae: 0.1230 - val_loss: 3.3264 - val_mae: 0.1260

Experiment 15 ,22 mar: 
-inghetat baza resnet50
-lambda noobj si coord schimbat la 1 si 5, obj la 1,cls 2
-schimbat no_grids=7-dense head 2048,leaky relu 0.1, drop 0.5
-learning_rate=1e-6 constant
-batch size 8
-64 epoci 
-start:
-end: loss:  6.5772 - mae: 0.1643 - val_loss: 6.3547 - val_mae: 0.1623
/// continuare fine tune
- dezghetat tot
- totu la fel 
- 192 epoci
- end:  loss: 2.3658 - mae: 0.1480 - val_loss: 5.5707 - val_mae: 0.1375

Experiment 16, 24.03:
- dezghetat tot, 2048 dense, leaky 0.1, drop 0.6, output linear
-no_grids 7
- schimbat functia loss: ptr bbox si confidence am pus tf.nn.sigmoid ca transformare si pentru clase am pus tf.nn.softmax, in rest am pastrat calculul loss-ului la fel
- batch 8 
- epoci 100
-lr 0.00001
- end: Epoch 00099: loss did not improve from 8.87421
Epoch 100/250
1542/1542 [==============================] - 459s 298ms/step - loss: 8.6825 - mae: 1.4237 - val_loss: 47.8476 - val_mae: 1.3022

!!!!BEST!!!!
Experiment 17, 24.03:
- dezghetat tot, 2048 dense, leaky 0.1, drop 0.6, output sigmoid
-no_grids 7
-train/test split 0.8/0.2
- functie loss normala cu sum peste toate imaginile din batch
- batch 16
- epoci 250
- lr 0.00001 ( cu 0.001,0.0001 nu a mers prea bine, scadea incet )
- end: 293s 428ms/step - loss: 3.3784 - mae: 0.1380 - val_loss: 68.3267 - val_mae: 0.1349
Epoch 00250: loss did not improve from 3.35610

OBS:- Desi val_loss-ul e mult mai mare decat loss-ul pe train, predictiile sunt foarte bune si precise in general. Class score-ul este destul de mic totusi si ar trebui sa ma concentrez sa obtin o eroare de clasificare mai mica. 
- inferenta se face cu 13-14FPS in medie
- am adaugt Non Maximum Suppression
Experiment 18,29.03:
- am schimbat lr= 0.000005
- am mai lasat 250 de epoci
- a scazut la 2.0 training loss ul si a crescut la 69 val loss ul
- nicio imbunatatire in predictii, actually sunt mai rele

Experiment 19,30.03:
- dezghetat doar head for warm up,2048 dense, leaky 0.1, drop 0.6, output sigmoid
- facut mean pe loss final la dimensiune batch
- init lr = 0.0001
-batch 32
-100 epoci
-end: slab: overfit imediat, nu trece de 5.7 val loss-ul 

Experiment 20,31.03:
- dezghetat doar head for warm up,4096 dense, leaky 0.1, drop 0.6, output sigmoid
- facut mean pe loss final la dimensiune batch
- init lr = 0.001(nan loss),0.0001(nu invata),reduce lr on plateau, patience= 6, factor = 0.1,metrics = val_loss
-batch 32
-100 epoci
-niciun rezultat ok in primele epoci

Experiment 21,31.03: (reincercat antrenat apropiat de cel mai bun model incercat)
- warm cu head doar, 2048 dense, !mish activation, drop 0.6, output sigmoid
-no_grids 7
-train/test split 0.72train/0.18val/0.1 test
- mean pe batch
- batch 16
- lr = 0.0001, reduce lr on plateau, patience= 6, factor = 0.1,metrics = val_loss,early stop patience 10
-early stop epoch 17 5.52 val_loss

Experiment 22,31.03: (reincercat antrenat apropiat de cel mai bun model incercat)
- warm cu head doar, 2048 dense, !mish activation, drop 0.6, output sigmoid
-no_grids 7
-train/test split 0.72train/0.18val/0.1 test
- mean pe batch
- batch 16
- lr = 0.0005(nan), reduce lr on plateau, patience= 6, factor = 0.2,metrics = val_loss,early stop patience 10
- batch 64+lr 0.00005 --end 5.15 val_loss, epoch ~65-70

Experiment 23,31.03: (reincercat antrenat apropiat de cel mai bun model incercat)
- warm cu head doar, 4096 dense, !mish activation, drop 0.5, output sigmoid
-no_grids 7
-train/test split 0.72train/0.18val/0.1 test
- mean pe batch
- batch 64
- lr = 0.00005, reduce lr on plateau, patience= 6, factor = 0.2,metrics = val_loss,early stop patience 10
- end: val_loss 5.05 epoch ~89, last learning rate 3*10^7
Experiment 24:
-dezghetat cap+un block( layers[-36:]=True)
-folosit warm 5.05 si ultima configuratie din exp 23
-lr = 0.0001 si tot restu la fel
-150 de epoci
-epoch 19 end 5.15 
Exp 25:
-batch 8
-lr = 0.00001
-end epoch 19:loss: 2.5616 - mae: 0.1365 - val_loss: 5.1225 - val_mae: 0.1342
Exp 26:
-batch 8 and lr = 0.000001
-epoch 25, loss: 3.9141 - mae: 0.1336 - val_loss: 5.2456 - val_mae: 0.1303
Exp 27:
-dezghetat doua blocuri [-98:],fol warm 5.05
-lr 0.0001(nan),batch 16(oom allocation error), batch 8
- lr 0.00001 and batch 8
-epoch 65 end:- 935s 607ms/step - loss: 0.4730 - mae: 0.1353 - val_loss: 2.9032 - val_mae: 0.1203
Exp 28:
-inghetat intreg resnet, 224 input
-leaky 0.2, batch 16,drop0.7, loss class_lambda=1,9 grids
-epoc <60 lr = 10^-5, epoch > 60 lr = 10^-6
-epoch 113:  loss: 3.2760 - mae: 0.1532 - val_loss: 3.9125 - val_mae: 0.1508
Continuare:
-micsorat batch la  8
-marit lr la 10^-5
-antrenat inca 100 epoci tot in warm up 
- STOP DUPA 10 epoci
- coborat la 10^-6
-reluat antrenare
-nah

Exp 29:
-4096, 9 grids, 16 batch, lr = 0.000001, leaky 0.3, drop 0.75
- loss: 3.1556 - mAP: 4.1682e-04 - val_loss: 3.8516 - val_mAP: 3.1271e-04
-overfit de la epoca 80, val loss stagnat de mult
Continuare:
-9 grids warm as load_model
- dezghetat de la blocu 3 tot si antrenat cu batch de 4 si ct lr  de 0.00001 250 epoci
-  epoch 250:876s 284ms/step - loss: 0.1864 - mAP: 0.0586 - val_loss: 3.7649 - val_mAP: 0.0277
- predictiile nu detecteaza nimic

Exp 30:
-warm cu 7 grids ptr 250 epoci cu linear output( TREBUIE SCHIMBAT SI IN PREDICT SI MAP OUTPUTURILE CU SIGMOID SI SOFTMAX)
- mAP 10% dupa fine tune 250 cu 4096 neuroni 7 grids

!!!!!!!!!!!!!!!!!! BUN
Exp 31:
- 7 grids, 2048-leaky 0.2-1024-leaky 0.2-sigmoid + in loss si mAP ptr class folosesc softmax peste sigmoid
- 300 epoci cu lr de 0.00001 ct si batch de 8
- in loss insumez noobject loss ul ptr ambele predictii -- vreau sa elimin erorile de detectie cat mai bine, multe predictii pot fi de la al doilea bbox care face predictii proaste si nu de la cel cu iou-ul cel mai bun.
- Epoch 300/300
1542/1542 [==============================] - 411s 266ms/step - loss: 1.8451 - mAP: 0.1942 - val_loss: 3.8271 - val_mAP: 0.0024
OBSERVATIE:
- pe setul de train overfit f bun, fara multe detectie proaste si majoritatea erorilor erau de clasificare
- modelul parea ca mai invata
- pe testare si validare nu detecteaza mai nimic, overfit adevarat

Exp 32:
-- adaugat drop de 0.6 la ultima retea
-- output linear cu sigmoid pe box si has_obj, softmax pe clase. Ptr class loss am schimbat si folosesc binary crossentropy: loss = label * (-1) * log(pred) + (1 - label) * (-1) * log(1 - pred)
-- antrenat 600 epoci lr 1e-5
-- am schimbat si in transforms si predict_pascal cu softmax si sigmoid
--epoch 120:  loss: 0.7638 - mAP: 0.0281 - val_loss: 10.8165 - val_mAP: 6.2402e-04

Exp 33:
-- 2048 l2 reg-leaky-drop 0.75-1024-drop 0.75
-- 600 epoci
-- lr 0.0001 apoi scazut la 0.00001
-- naspa map ul mic

Exp 34:
--- 2048 - leaky 0.2 - drop 0.7 - linear output, sum over loss si restu de schimbari cu softmax etc.
--- lr = 0.0001
--- batch = 16
--- 515 epochs
--- loss: 4.1148 - mAP: 0.1002 - val_loss: 132.7442 - val_mAP: 0.0104
--- map ul inca crestea dar incet

Exp 35:
--- 2048, leaky 0.1 - drop 0.6 - sigmoid , loss - bce la class si in loc de mse am pus mae
--- 800 epoci, lr = 0.00005
--- naspa dupa 70 de epoci overfit deja

Exp 36:
---resnet50 feature extractor doar + sigmoid, loss normal dar cu tf.nn.L2_loss, batch 4 cu  layers unfrozen de la 39 in sus
---pana la epoca 76 cu 0.0001(mai mare dadea nan)
--Epoch 00076: mAP improved from 0.27146 to 0.27199, saving model to output/best_model_retry.hdf5
Epoch 77/150
3083/3083 [==============================] - 522s 169ms/step - loss: 0.0629 - mAP: 0.2770 - val_loss: 1.8692 - val_mAP: 0.0175
--- fol 0.00001 inca 85 de epoci
Epoch 00084: mAP did not improve from 0.31372
Epoch 85/150
3083/3083 [==============================] - 529s 171ms/step - loss: 0.0060 - mAP: 0.3102 - val_loss: 1.8800 - val_mAP: 0.0172

Exp 37:
1)warm
- 0.85 train si 0.15 validation, fara test set
- conv2d(1024,3,1,no_pad,l2_red,henormal_init)-leaky0.1-BN-drop0.4-conv2d(512,3,1,no_pad,l2_red,henormal_init)-leaky0.1-BN-drop0.4-flatten-dense_sigmoid
- lr 0.0001 pana la 30, 30-55 0.00005, >55 0.00001
- batch 32
Epoch 150/150
455/455 [==============================] - 144s 316ms/step - loss: 1.7240 - mAP: 0.0012 - val_loss: 2.1576 - val_mAP: 0.0018
-- pe la epoca 60 incepe overfit pe warm =>>> as putea lasa cu 0.00005 mai mult timp
2)fiine tune
-- same network unfreeze de la 39 incolo(de la al 3-lea block)
--batch 8
-- lr 0.0001, folosit ctrl+c method

OBSERVATIE:
-adaugat augmentare pe imagini si neaparat reincercat reteaua cea mai buna sa vad cat mAP are de fapt
NEXT:
- reincercat cu lr de 0.001 si 0.0001 sa vad daca merge in nan si sa scad treptat lr-ul cu Reduce on Plateau
-marit training/test ratio 0.9/0.1 (DONE)
-regularizari
-sa antrenez ptr 250 de epoci si cu 9/11 griduri
-sa antrenez mai mult de 250 de epoci cu aceiasi configuratie
-schimb din sum ptr toate imaginile in MEAN !!
-sa incerc sa reduc overfit-ul
-sa adaug mAP, sa imbunatatesc algoritmul de NMS
-sa adaug ancore
-
-incercat cu 4096 neuroni
- facut warm si obtinut un model cu loss cat mai mic si dupa fine tune si dezghetat pe rand cate un bloc/doua/trei/toate reteaua

OBS: AM SCHIMBAT IN TRANSFORMS DE LA 25 la 30 forma sa fie la fel ca de la 21 la 25
Links:

https://medium.com/@m.khan/implementing-yolo-using-resnet-as-feature-extractor-5857f9da5014

https://github.com/makatx/YOLO_ResNet/blob/8f7b593e5232f8038a72eb52e920a97c207dd57c/model_continue_train.py#L294

https://github.com/thtrieu/darkflow/issues/9
